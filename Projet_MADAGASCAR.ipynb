{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Enjeux économiques, sociaux et environnementaux à Madagascar: Cas pratique en Data Science"
      ],
      "metadata": {
        "id": "FcjiNaKaA8w8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Amélioration de l’accès aux données ouvertes à Madagascar: Création d’une base de données réutilisable des entreprises créées à Madagascar\n",
        "\n"
      ],
      "metadata": {
        "id": "FpHRSBFl_565"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction et problématiques\n",
        "\n",
        "Madagascar est un pays où l’accès aux données ouvertes est difficile. Des associations comme Openstat proposent déjà des listes d’entreprises au format réutilisable, mais ces listes ne sont pas exhaustives car elles ne reprennent que les entreprises créées au sein de l’EDBM1. Dans le cadre du projet “Analyse des enjeux économiques et sociaux à Madagascar”, nous avons entrepris un sous-projet visant à améliorer l’accès aux données ouvertes en créant une base de données réutilisable des entreprises créées à Madagascar. Cette base de données comprendra des informations telles que le type d’activité des entreprises et d’autres informations pertinentes. Nous prévoyons de récupérer toutes les entreprises enregistrées sur le Registre du Commerce et des Sociétés de Madagascar2 ainsi que d’autres sources comme des annuaires3. Pour ce faire, nous utiliserons des techniques telles que le web scraping et l’apprentissage automatique pour extraire et traiter les données. Nous prévoyons également de proposer nos données à des associations telles que Openstat1 ou des blogs d’annuaire comme Agoramada4 qui souhaitent améliorer les données ouvertes à Madagascar. Ce projet nous permet également de nous améliorer grandement dans le nettoyage et le traitement des données, qui sont les processus les plus difficiles dans l’analyse de données."
      ],
      "metadata": {
        "id": "irBV8U_7cxTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web scraping\n",
        "Le web scraping est le processus d'extraction automatique d'informations et de données à partir de pages web. Cela implique l'utilisation de programmes informatiques pour naviguer sur des sites web, extraire le contenu, analyser la structure des pages et récupérer les données pertinentes. Le but du web scraping est de collecter et d'organiser les données à partir de différentes sources en ligne pour une utilisation ultérieure, comme l'analyse de données, la visualisation, la recherche, la surveillance, etc.\n",
        "\n",
        "On va récupérer les données directement sur internet. On va prenre la liste des sociétés créées à l'EDBM en 2017. On va utiliser la bibliothèque Beautiful Soup. Beautiful Soup est une bibliothèque Python largement utilisée pour analyser et extraire des informations à partir de documents HTML et XML. En data analyse, Beautiful Soup permet de traiter et d'organiser les données contenues dans des pages web, en les transformant en structures de données compréhensibles par Python.\n",
        "\n",
        "Ici, on va utiliser principalement deux bibliothèques: *Beautiful Soup* et *Requests*. Request  est utilisée pour effectuer des requêtes HTTP vers des serveurs web et récupérer le contenu HTML des pages. Elle permet d'obtenir le code source brut des pages web. Cela peut être comparé à ouvrir un lien dans un navigateur et voir le code source de la page.\n",
        "Une fois qu'on a le contenu HTML de la page à partir de \"requests\", Beautiful Soup entre en jeu. Beautiful Soup analyse le contenu HTML et le transforme en une structure d'objet Python avec laquelle on peut interagir facilement. Elle fournit des méthodes pour extraire des données, rechercher des balises, accéder aux attributs et au contenu, et naviguer dans la structure du document HTML."
      ],
      "metadata": {
        "id": "skrHoDoy5CkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importation des bibliothèques"
      ],
      "metadata": {
        "id": "2h4SXWE8I808"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "G1gxIKWwJLBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilisation de BeautifulSoup avant Panda pour plus de précision"
      ],
      "metadata": {
        "id": "I0xcw8aBStap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url=\"https://edbm.mg/actualites_statistiques_liste-des-societes-creees-a-l-edbm/\"\n",
        "\n",
        "page=requests.get(url)  # la fonction  envoie une requête GET à l'URL spécifiée au serveur web correspondant et le serveur web répond à la requête en renvoyant les données associées à cette URL\n"
      ],
      "metadata": {
        "id": "vhWKmI7iBWts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va maintenant créer un objet BeautifulSoup. Un objet Beautiful Soup est une structure de données créée à l'aide de la bibliothèque Beautiful Soup en Python. Un objet Beautiful Soup est un moyen pratique de transformer le contenu HTML ou XML d'une page web en une structure d'objet Python avec laquelle vous pouvez interagir facilement pour extraire des informations et effectuer des manipulations.\n",
        "\n",
        "**page.text** permet d'obtenir le contenu sous forme de chaîne de caractères.\n",
        "\n",
        "**html.parser** est le nom du parseur qu'on utilise pour analyser le contenu HTML. Un parseur est un programme qui analyse une chaîne de caractères en fonction d’une grammaire spécifiée et construit une structure de données représentant la structure logique de la chaîne."
      ],
      "metadata": {
        "id": "P846nyuB7DKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup=BeautifulSoup(page.text,\"html.parser\")  #BeautifulSoup s'attend à recevoir une chaîne de caractère\n",
        "#soup\n",
        "\n",
        "table=soup.find_all(\"table\")[0]  #On cherche à afficher la première table mais s'il n'y a qu'une table, on peut juste faire soup;find(\"table\")"
      ],
      "metadata": {
        "id": "LaC1SGFL9iaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_html(str(table))  #On convertit la table en chaîne de caractère\n",
        "#df[0]"
      ],
      "metadata": {
        "id": "9CS893pLR5ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La méthode read_html de la bibliothèque pandas peut être utilisée pour extraire des données tabulaires à partir d’un document HTML. Cette méthode prend en entrée une chaîne de caractères contenant du code HTML et renvoie une liste de DataFrames, chacun contenant les données d’une table HTML trouvée dans le document.\n",
        "\n",
        "Cependant, dans certains cas, on peut avoir besoin de plus de contrôle sur la manière dont les données sont extraites du document HTML. Par exemple, si le document contient plusieurs tables et qu'on ne veut extraire les données que d’une seule table spécifique, ou si on veut prétraiter les données avant de les charger dans un DataFrame. Dans ces cas, on peut utiliser la bibliothèque BeautifulSoup pour analyser le document HTML et extraire les données de la table que l'on veut.\n",
        "\n",
        "En utilisant BeautifulSoup, on peut parcourir le document HTML et sélectionner la table que l'on veut en utilisant des sélecteurs CSS ou des méthodes de recherche. Une fois que l'on a trouvé la table, on peut la convertir en chaîne de caractères en utilisant la fonction str et passer cette chaîne à la méthode read_html de pandas pour créer un DataFrame.\n",
        "\n",
        "En résumé, l’utilisation de BeautifulSoup avant d’utiliser pandas peut nous donner plus de contrôle sur la manière dont les données sont extraites du document HTML. Si on n’a pas besoin de ce niveau de contrôle, on peut utiliser directement la méthode read_html de pandas pour extraire les données des tables HTML."
      ],
      "metadata": {
        "id": "pudU7UL4RdkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilisation directe de Panda sans passer par BeautifulSoup\n"
      ],
      "metadata": {
        "id": "EfJdt2KeS9km"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant, on va juste utiliser Panda pour extraire les tables de la page"
      ],
      "metadata": {
        "id": "pZDYxQjHSKLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url=\"https://edbm.mg/actualites_statistiques_liste-des-societes-creees-a-l-edbm/\"\n",
        "df = pd.read_html(url,header=0)  # Renvoye une liste de dataframe, qui sont les tableux de la page\n",
        "\n",
        "# Vu que df est une liste, pour selectionner le premier tableau, on va faire comme ceci:\n",
        "#df[0]"
      ],
      "metadata": {
        "id": "cngjgdcbMq8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lswx60BqMrAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application sur la plus grande base de données répertoriant les sociétés auprès de RNCS"
      ],
      "metadata": {
        "id": "YdKH4kYggUWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNCS-CM Madagascar est le Registre National du Commerce et des Sociétés de Madagascar.\n",
        "Il a pour but de recueillir et de publier des informations juridiquement importantes relatives aux commerçants et aux personnes morales assujetties à l’immatriculation. C’est un outil pour la transparence des entreprises, accessible à tous"
      ],
      "metadata": {
        "id": "WE_FHTD6h72v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url=\"https://www.rcsmada.mg/index.php?pgdown=consultation&soc=1-1#\"\n",
        "\n",
        "page=requests.get(url)\n",
        "soup=BeautifulSoup(page.text)\n",
        "#soup"
      ],
      "metadata": {
        "id": "8mZddWONaI_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1er problème rencontré\n"
      ],
      "metadata": {
        "id": "Ydk7p11N4FIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lorsque je copie le lien de la page contenant le tableau, le tableau n'apparaît pas dans le nouvel onglet. On est donc obligé d'automatiser les tâches en utilisant la bibliothèque Selenium pour appuyer automatiquement sur le bouton \"Rechercher\" afin d'afficher le tableau contenant la liste des entreprises. Selenium est souvent utilisé pour automatiser l'interaction avec des pages web dynamiques. Les pages web dynamiques sont celles qui changent ou se mettent à jour en réponse à des actions de l'utilisateur, telles que des clics de boutons, des remplissages de formulaires, des défilements, etc. Cela peut également inclure des pages qui utilisent des technologies comme JavaScript pour modifier le contenu de la page sans nécessiter de rechargement complet.\n",
        "\n",
        "Selenium permet de simuler ces interactions et actions de l'utilisateur, ce qui en fait un outil précieux pour automatiser le test de sites web, la récupération de données à partir de pages web dynamiques, et d'autres tâches similaires.\n",
        "\n",
        "En utilisant Selenium, vous pouvez automatiser des tâches comme cliquer sur des boutons, remplir des formulaires, faire défiler la page, attendre que certaines parties de la page se chargent, et même capturer des captures d'écran à différentes étapes du processus. Cela en fait une option populaire pour les développeurs, les testeurs et les professionnels du web scraping qui ont besoin d'interagir avec des pages web complexes."
      ],
      "metadata": {
        "id": "ZZIhXVhi4LCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilisation de Selinium"
      ],
      "metadata": {
        "id": "ar9nU8NoEaBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selenium\n",
        "\n",
        "Selenium est une bibliothèque qui permet de naviguer sur une page web de façon automatisée et donc de mettre en place divers degrés d’interaction"
      ],
      "metadata": {
        "id": "QHAKfPDL2Vg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explication du driver\n"
      ],
      "metadata": {
        "id": "jCHYuOGbFOhw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "un driver est une interface entre votre code de test (généralement écrit en Python, Java, C#, etc.) et le navigateur que vous souhaitez contrôler automatiquement. Le driver agit comme un pont de communication entre votre script et le navigateur, ce qui permet à votre script de simuler des actions d'utilisateur, telles que le clic sur des boutons, la saisie de texte et la navigation.\n",
        "\n",
        "Chaque navigateur (comme Chrome, Firefox, Edge, etc.) a son propre driver spécifique qui doit être utilisé avec Selenium pour automatiser ce navigateur particulier. Les drivers permettent à Selenium de contrôler le navigateur en simulant des interactions humaines, ce qui est particulièrement utile pour tester des applications web, extraire des données de pages web ou effectuer d'autres tâches automatisées.\n",
        "\n",
        "Lorsque vous utilisez Selenium, vous devez télécharger le driver correspondant au navigateur que vous souhaitez contrôler, et vous devez également installer la bibliothèque Selenium pour le langage de programmation que vous utilisez (comme selenium en Python). Ensuite, vous utiliserez le driver pour ouvrir une instance du navigateur et effectuer des actions automatisées.\n",
        "\n",
        "Par exemple, si vous utilisez le navigateur Chrome, vous devrez télécharger le ChromeDriver et l'utiliser avec la bibliothèque Selenium pour contrôler les interactions avec le navigateur Chrome. De même, pour Firefox, vous devrez utiliser le GeckoDriver, et ainsi de suite"
      ],
      "metadata": {
        "id": "OxQu2cRGFOrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!apt-get update      : met à jour la liste des paquets disponibles pour l’installation\n",
        "#!apt install chromium-chromedriver  # Installation du driver chrome\n",
        "#!cp /usr/lib/chromium-browser/chromedriver /usr/bin   #Le répertoire /usr/bin est un répertoire système qui contient des programmes exécutables accessibles à tous les utilisateurs. En copiant le pilote Chrome dans ce répertoire, vous vous assurez que Selenium peut le trouver et l’utiliser pour contrôler Chrome\n",
        "!pip install selenium"
      ],
      "metadata": {
        "id": "OnTqzNRnaJB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importation des classes et modules nécessaires\n",
        "from selenium.webdriver.common.by import By  # Pour identifier les éléments sur une page web\n",
        "from selenium import webdriver  # Classe de base pour automatiser les navigateurs\n",
        "import pandas as pd  # Pour la manipulation de données tabulaires\n",
        "from time import sleep  # Pour introduire des délais\n",
        "\n",
        "# Configuration des options du navigateur\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')  # Exécution sans afficher l'interface graphique du navigateur\n",
        "options.add_argument('--no-sandbox')  # Options de sécurité pour l'exécution\n",
        "options.add_argument('--disable-dev-shm-usage')  # Options pour gérer l'utilisation de la mémoire\n",
        "driver = webdriver.Chrome(options=options)  # Création de l'instance du navigateur Chrome avec les options\n",
        "\n",
        "# Ouvrir la page web\n",
        "page = driver.get(\"https://www.rcsmada.mg/index.php?pgdown=liste2\")\n",
        "\n",
        "# Trouver et cliquer sur le lien \"Liste d'Entreprise\"\n",
        "lien_liste_entreprises = driver.find_element(By.LINK_TEXT, \"Liste d'Entreprise\")\n",
        "lien_liste_entreprises.click()\n",
        "\n",
        "# Attendre pendant 5 secondes pour que la page se charge complètement\n",
        "sleep(5)\n",
        "\n",
        "# Extraire le code source HTML de la page chargée\n",
        "html = driver.page_source\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nbskmzr3aJGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "un objet WebElement n’est pas le code HTML de l’élément, mais plutôt une représentation d’un élément HTML dans la page web. Un objet WebElement fournit des méthodes pour interagir avec l’élément, comme click, send_keys et submit, ainsi que des méthodes pour récupérer les attributs et le texte de l’élément, comme get_attribute et text. Si vous voulez récupérer le code HTML d’un élément WebElement, vous pouvez utiliser la méthode get_attribute avec l’attribut 'outerHTML'.\n",
        "\n",
        "outerHTML est un attribut de l’interface DOM Element qui permet de récupérer le fragment HTML sérialisé décrivant l’élément ainsi que ses descendants1. Il peut également être utilisé pour remplacer l’élément avec les nœuds générés à partir de la chaîne fournie1. En d’autres termes, outerHTML renvoie une chaîne contenant le code HTML de l’élément, y compris ses balises de début et de fin, ainsi que le code HTML de tous ses éléments enfants1.\n",
        "\n",
        "Lorsque vous utilisez la méthode get_attribute avec l’attribut 'outerHTML', vous récupérez le code HTML de l’élément WebElement sous forme de chaîne. Vous pouvez ensuite utiliser cette chaîne pour effectuer du web scraping avec pandas ou une autre bibliothèque."
      ],
      "metadata": {
        "id": "q-LKrTZGpJO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On va selectionner les valeurs du champ Par Greffe RCS\n",
        "\n",
        "bouton_rechercher=driver.find_element(By.XPATH,'//input[@type=\"submit\" and @value=\"Rechercher\"]')\n",
        "\n",
        "bouton_rechercher.click()\n",
        "sleep(5)"
      ],
      "metadata": {
        "id": "gil0b5NlaJKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**//input:** Sélectionne tous les éléments input dans la page, quel que soit leur emplacement dans la hiérarchie du document.\n",
        "\n",
        "**[@type=\"submit\"]:** Restreint la sélection aux éléments input dont l’attribut type est égal à “submit”.\n",
        "and: Combine les deux conditions précédentes en utilisant l’opérateur logique “et”.\n",
        "\n",
        "**[@value=\"Rechercher\"]:** Restreint la sélection aux éléments input dont l’attribut value est égal à “Rechercher”.\n",
        "\n",
        " Le symbole @ est utilisé pour indiquer que vous faites référence à un attribut d’un élément dans un chemin d’accès XPath."
      ],
      "metadata": {
        "id": "-JBMxxuib-wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "html_liste_ambositra=driver.page_source\n",
        "\n",
        "df=pd.read_html(html_liste_ambositra)[0]\n",
        "#df\n"
      ],
      "metadata": {
        "id": "d5Ki4rsvb9mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2eme problème rencontré\n",
        "\n",
        "On a réussi à récupérer le tableau, mais ce qui nous intéresse, c'est ce qu'il y a dans *+d'Infos*. Il y a plus de détails donc on va éssayer de récupérer les tableaux se trouvant dans *+d'infos* pour chaque ligne."
      ],
      "metadata": {
        "id": "KlO6YSXrMpA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "links = driver.find_elements(By.XPATH,'//a[text()=\"+ d\\'Infos\"]')\n",
        "# Initialiser le DataFrame resultat\n",
        "resultat = pd.DataFrame()\n",
        "for link in links:\n",
        "    # Récupérer l'URL du lien\n",
        "    href = link.get_attribute('href')\n",
        "    # Lire le tableau HTML à partir de l'URL\n",
        "    df = pd.read_html(href)[0]\n",
        "\n",
        "    #Définir la première colonne comme index\n",
        "    df=df.set_index(df.columns[0])\n",
        "\n",
        "    # Transposer le DataFrame\n",
        "\n",
        "    df=df.transpose()\n",
        "\n",
        "    # Vérifier si le DataFrame resultat est vide\n",
        "    if resultat.empty:\n",
        "        # Si oui, initialiser le DataFrame resultat avec le premier DataFrame\n",
        "        resultat = df\n",
        "    else:\n",
        "        # Sinon, effectuer la jointure avec le DataFrame courant\n",
        "        common_columns = list(set(resultat.columns) & set(df.columns))\n",
        "        resultat=pd.merge(resultat,df,on=common_columns,how=\"outer\")\n"
      ],
      "metadata": {
        "id": "2oynCSJfUQJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le critère XPATH utilisé ici est '//a[text()=\"+ d\\'Infos\"]'. Cela signifie que nous recherchons tous les éléments \"a\" (liens hypertextes) sur la page Web dont le texte est égal à \"+ d'Infos\". En d’autres termes, nous recherchons tous les liens sur la page Web qui ont le texte \"+ d'Infos\".\n",
        "\n",
        "**text()** n’est pas un attribut dans ce cas. text() est une fonction XPATH qui renvoie le texte contenu dans un élément. Dans l’expression XPATH '//a[text()=\"+ d\\'Infos\"]', la fonction text() est utilisée pour sélectionner tous les éléments \"a\"dont le texte est égal à \"+ d'Infos\".\n",
        "\n",
        "Dans un chemin XPATH, un attribut est une caractéristique d’un élément, tandis qu’une fonction est une opération qui peut être effectuée sur un élément ou un ensemble d’éléments.\n",
        "\n",
        "Un attribut est spécifié en utilisant la syntaxe @attribut dans un chemin XPATH. Par exemple, pour sélectionner tous les éléments \"a\" qui ont un attribut href éga à \"https://www.example.com\", vous pouvez utiliser l’expression XPATH suivante: //a[@href=\"https://www.example.com\"].\n",
        "\n",
        "Les fonctions XPATH, en revanche, sont des opérations qui peuvent être effectuées sur des éléments ou des ensembles d’éléments. Par exemple, la fonction text() renvoie le texte contenu dans un élément, tandis que la fonction count() renvoie le nombre d’éléments dans un ensemble d’éléments."
      ],
      "metadata": {
        "id": "C7yQVCFVoEvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#resultat"
      ],
      "metadata": {
        "id": "C9-OR0w5aJTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a réussi à afficher la liste des entreprises d'une ville sur la première page. La liste s'étend sur plusieurs pages donc on va devoir parcourir toutes les pages pour récuperer toutes les entreprises.\n",
        "\n",
        "**On va supprimer les accents dans les noms des colonnes pour ne pas causer d'erreurs lors de la jointure des dataframes. Pour cela, on va utiliser la bibliothèque unidecode**"
      ],
      "metadata": {
        "id": "NpYUnqkYq-tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "id": "lI1cAcP9gKi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unidecode import unidecode\n",
        "import chardet\n",
        "import urllib.request\n"
      ],
      "metadata": {
        "id": "pMSnQSaYuSwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "liens_pages=driver.find_elements(By.XPATH,\"//span[@class='butons']/a\")\n",
        "\n",
        "#On stocke tous les liens comme ça on n'a pas besoin d'utiliser la méthode click qui cause souvent des erreurs\n",
        "liens_href=[]\n",
        "for lien in liens_pages:\n",
        "  liens_href.append(lien.get_attribute('href'))\n",
        "\n",
        "#On peut simplifier l'écriture comme ceci:\n",
        "#liens_href = [lien.get_attribute('href') for lien in liens_pages]\n",
        "\n",
        "\n",
        "resultat = pd.DataFrame()\n",
        "for lien_href in liens_href:\n",
        "  driver.get(lien_href)\n",
        "  links = driver.find_elements(By.XPATH,'//a[text()=\"+ d\\'Infos\"]')\n",
        "  # Initialiser le DataFrame resultat\n",
        "\n",
        "  for link in links:\n",
        "      # Récupérer l'URL du lien\n",
        "      href = link.get_attribute('href')\n",
        "\n",
        "      # Lire les données brutes à partir de l'URL\n",
        "      rawdata = urllib.request.urlopen(href).read()\n",
        "\n",
        "      # Détecter l'encodage des données brutes\n",
        "\n",
        "      encoding = chardet.detect(rawdata)['encoding']\n",
        "\n",
        "      # Lire le tableau HTML à partir de l'URL en utilisant l'encodage détecté\n",
        "      df = pd.read_html(href, encoding=encoding)[0]\n",
        "\n",
        "\n",
        "\n",
        "      #Définir la première colonne comme index\n",
        "      df=df.set_index(df.columns[0])\n",
        "\n",
        "      # Transposer le DataFrame\n",
        "      df=df.transpose()\n",
        "\n",
        "\n",
        "      # Supprimer les colonnes en double sauf la première\n",
        "      df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
        "\n",
        "      #supprimer les accents dans les noms des colonnes\n",
        "      df.columns = df.columns.map(unidecode)\n",
        "\n",
        "      # Vérifier si le DataFrame resultat est vide\n",
        "      if resultat.empty:\n",
        "          # Si oui, initialiser le DataFrame resultat avec le premier DataFrame\n",
        "          resultat = df\n",
        "      else:\n",
        "          # Sinon, effectuer la jointure avec le DataFrame courant\n",
        "          common_columns = list(set(resultat.columns) & set(df.columns))\n",
        "          resultat=pd.merge(resultat,df,on=common_columns,how=\"outer\")\n"
      ],
      "metadata": {
        "id": "2tbimX5GQadO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "856havWEdJJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Important: .get() vs .click()\n",
        "\n",
        "**La méthode .get** charge une nouvelle page Web dans la fenêtre du navigateur en effectuant une requête HTTP GET vers l’URL spécifiée. La méthode bloque jusqu’à ce que la page soit complètement chargée.\n",
        "\n",
        "**La méthode .click()** ne bloque pas en attendant que la nouvelle page soit chargée, elle renvoie immédiatement le contrôle à votre script.\n",
        "\n"
      ],
      "metadata": {
        "id": "otx4ob_X-rEw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-J69LrNwQai6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apprentissage automatique supervisé VS apprentissage non supervisé\n",
        "\n",
        "\n",
        "Dans cette partie, on va essayer de regrouper les entreprises par type d'activités. On n'a que les descriptions des activités des des entreprises sans le type donc on va utiliser l'apprentissage automatique pour les regrouper. On va d'abord le tester sur une petite base de données."
      ],
      "metadata": {
        "id": "BCVZFblgOarJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apprentissage automatique non supervisé\n",
        "\n",
        "Premièrement, utilisons le clustering pour regrouper les activités automatiquement."
      ],
      "metadata": {
        "id": "Y9XXwffisZyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "8uRfjbWuRJ9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ambositra=pd.read_csv(\"/content/ambositra_encodage_automatique.txt\")\n"
      ],
      "metadata": {
        "id": "0B4-yrPsRKAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colonne_activite=ambositra[\"Activite\"]"
      ],
      "metadata": {
        "id": "OUCvDs78Y1xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colonne_activite"
      ],
      "metadata": {
        "id": "uWmNwJ-DuWis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listes_ambositra=list(colonne_activite)"
      ],
      "metadata": {
        "id": "lujfhDZMY15n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listes_ambositra"
      ],
      "metadata": {
        "id": "WH6xd9ivY1-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorisation par TF-IDF"
      ],
      "metadata": {
        "id": "ivRyHBFTvS5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "# Les descriptions des activités des entreprises\n",
        "descriptions = listes_ambositra\n",
        "\n",
        "# Supprimer les valeurs NaN de la liste\n",
        "descriptions = [desc for desc in descriptions if not isinstance(desc, float) or not math.isnan(desc)]\n",
        "\n",
        "\n",
        "# Prétraitement des descriptions (en minuscules, suppression de la ponctuation)\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "preprocessed_descriptions = [preprocess_text(desc) for desc in descriptions]\n",
        "\n",
        "# Vectorisation TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(preprocessed_descriptions)\n",
        "\n",
        "# Clustering KMeans\n",
        "num_clusters = 21  # ajuster ce nombre en fonction des cas\n",
        "kmeans = KMeans(n_clusters=num_clusters)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Assigner chaque description à un cluster\n",
        "cluster_assignments = kmeans.labels_\n",
        "\n",
        "# Afficher les descriptions groupées par cluster\n",
        "clusters = {}\n",
        "for i, cluster_id in enumerate(cluster_assignments):\n",
        "    if cluster_id not in clusters:\n",
        "        clusters[cluster_id] = []\n",
        "    clusters[cluster_id].append(descriptions[i])\n",
        "\n",
        "for cluster_id, cluster_items in clusters.items():\n",
        "    print(f\"Cluster {cluster_id + 1}:\\n\")\n",
        "    for item in cluster_items:\n",
        "        print(item)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "Xg6hzGLNRKHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jzgO9q6ZRKKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kTpeoY9ORKMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BERT\n"
      ],
      "metadata": {
        "id": "tTPmbfvVj2JQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "XKSsQg4OkMg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mn3mHmcQ7Ugs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.cluster import KMeans\n",
        "import math\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "skkmELHO6nh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Charger le tokenizer et le modèle BERT pré-entraînés\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
        "\n",
        "\n",
        "\n",
        "# Les descriptions des activités des entreprises\n",
        "descriptions = listes_ambositra\n",
        "\n",
        "# Supprimer les valeurs NaN de la liste\n",
        "descriptions = [desc for desc in descriptions if not isinstance(desc, float) or not math.isnan(desc)]\n",
        "\n",
        "# Prétraitement des descriptions (en minuscules, suppression de la ponctuation)\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "preprocessed_descriptions = [preprocess_text(desc) for desc in descriptions]\n",
        "\n",
        "# Encodage des descriptions avec BERT\n",
        "encoded_descriptions = []\n",
        "for desc in preprocessed_descriptions:\n",
        "    input_ids = tokenizer.encode(desc, add_special_tokens=True)\n",
        "    input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids)\n",
        "        encoded_desc = output[0][:, 0, :].numpy()\n",
        "        averaged_encoded_desc = np.mean(encoded_desc, axis=1)  # Aplatir en moyennant\n",
        "        encoded_descriptions.append(averaged_encoded_desc)\n",
        "\n"
      ],
      "metadata": {
        "id": "-p9sUbPZRKPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FsSsA6fQ7Cz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n"
      ],
      "metadata": {
        "id": "hourryfG7Xoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import CamembertTokenizer, CamembertModel\n",
        "\n",
        "\n",
        "# Charger le tokenizer et le modèle CamemBERT pré-entraînés\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "model = CamembertModel.from_pretrained('camembert-base')\n",
        "\n",
        "# Les descriptions des activités des entreprises\n",
        "descriptions = listes_ambositra\n",
        "\n",
        "# Supprimer les valeurs NaN de la liste\n",
        "descriptions = [desc for desc in descriptions if not isinstance(desc, float) or not math.isnan(desc)]\n",
        "\n",
        "# Prétraitement des descriptions (en minuscules, suppression de la ponctuation)\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "preprocessed_descriptions = [preprocess_text(desc) for desc in descriptions]\n",
        "\n",
        "# Encodage des descriptions avec CamemBERT\n",
        "encoded_descriptions = []\n",
        "for desc in preprocessed_descriptions:\n",
        "    input_ids = tokenizer.encode(desc, add_special_tokens=True)\n",
        "    input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids)\n",
        "        encoded_desc = output[0][:, 0, :].numpy()\n",
        "        averaged_encoded_desc = np.mean(encoded_desc, axis=1)  # Aplatir en moyennant\n",
        "        encoded_descriptions.append(averaged_encoded_desc)\n"
      ],
      "metadata": {
        "id": "LTG-r78T7C4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zzwtlG0X7C9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "# Clustering KMeans\n",
        "num_clusters = 5  # Vous pouvez ajuster ce nombre en fonction de votre cas\n",
        "kmeans = KMeans(n_clusters=num_clusters)\n",
        "kmeans.fit(encoded_descriptions)\n",
        "\n",
        "# Assigner chaque description à un cluster\n",
        "cluster_assignments = kmeans.labels_\n",
        "\n",
        "# Afficher les descriptions groupées par cluster\n",
        "clusters = {}\n",
        "for i, cluster_id in enumerate(cluster_assignments):\n",
        "    if cluster_id not in clusters:\n",
        "        clusters[cluster_id] = []\n",
        "    clusters[cluster_id].append(descriptions[i])\n",
        "\n",
        "for cluster_id, cluster_items in clusters.items():\n",
        "    print(f\"Cluster {cluster_id + 1}:\\n\")\n",
        "    for item in cluster_items:\n",
        "        print(item)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "j-yXB-vIRKUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Er-OBSVIRKYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oyuXdLCXRKbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importation des données CSV"
      ],
      "metadata": {
        "id": "SrcZ06QNUQsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "#my_local_drive='/content/gdrive/My Drive/Colab Notebooks/Madagascar'"
      ],
      "metadata": {
        "id": "z8NrijB9UYy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ch15FE6QVNzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importation des données\n",
        "\n"
      ],
      "metadata": {
        "id": "pve09lLYIdko"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjJLcO-eIq_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SqJZme6gtGD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xxm7hHVfu1ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6M0oKWUmu1cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RUS5LMlxQDRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "feEwANNgQDUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ZfCrtR8St_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FYBzfu4VRuCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C5DuWmwQRuEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6FfEo5X4RuJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c9Lft6BcU7JV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ml1PgsXwHcgG"
      }
    }
  ]
}